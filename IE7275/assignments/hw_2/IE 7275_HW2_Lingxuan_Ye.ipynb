{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "from collections import Counter, namedtuple\n",
    "from scipy.stats import multivariate_normal\n",
    "from sklearn import metrics\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "from sklearn.preprocessing import scale\n",
    "import statsmodels.api as sm\n",
    "\n",
    "\n",
    "Result = namedtuple(\"Result\", (\"predicted\", \"actual\"))\n",
    "TP = Result(1, 1)\n",
    "FP = Result(1, 0)\n",
    "TN = Result(0, 0)\n",
    "FN = Result(0, 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv(\"admission-1.csv\").query(\"De == 'admit' | De == 'notadmit'\")\n",
    "# `sklearn.naive_bayes.CategoricalNB` does not support negative value (part c)\n",
    "data[data.loc[:, \"De\"] == \"admit\"] = 1\n",
    "data[data.loc[:, \"De\"] == \"notadmit\"] = 0\n",
    "data.loc[:, \"De\"] = data.loc[:, \"De\"].astype(\"int\")\n",
    "\n",
    "# annotation for python 3.9 and above\n",
    "# for 3.8 and under, please use `typing.List` and `typing.Dict`\n",
    "cv_data: list[dict[str, pd.DataFrame]] = []\n",
    "for itrain, itest in KFold(shuffle=True).split(data):\n",
    "    cv_data.append(\n",
    "        {\n",
    "            \"train_x\": data.loc[itrain, [\"GPA\", \"GMAT\"]],\n",
    "            \"train_cls\": data.loc[itrain, \"De\"],\n",
    "            \"test_x\":data.loc[itest, [\"GPA\", \"GMAT\"]],\n",
    "            \"test_cls\": data.loc[itest, \"De\"]\n",
    "        }\n",
    "    )\n",
    "\n",
    "\n",
    "def estimate(index: int, estimator, epilog: str = None) -> None:\n",
    "    train_x: pd.DataFrame = cv_data[index][\"train_x\"]\n",
    "    train_cls: pd.DataFrame = cv_data[index][\"train_cls\"]\n",
    "    test_x: pd.DataFrame = cv_data[index][\"test_x\"]\n",
    "    test_cls: pd.DataFrame = cv_data[index][\"test_cls\"]\n",
    "\n",
    "    estimator.fit(train_x, train_cls)\n",
    "    predicted_cls: np.ndarray = estimator.predict(test_x)\n",
    "\n",
    "    counter: Counter[tuple[str, str], int] = Counter(\n",
    "        (Result(i, j) for i, j in zip(predicted_cls, test_cls))\n",
    "    )\n",
    "\n",
    "    matrix = pd.DataFrame(\n",
    "        index=[\"Predicted Admit\", \"Predicted NotAdmit\"],\n",
    "        columns=[\"Actual Admit\", \"Actual NotAdmit\"]\n",
    "    )\n",
    "    matrix.iloc[0, 0] = counter[TP]\n",
    "    matrix.iloc[0, 1] = counter[FP]\n",
    "    matrix.iloc[1, 0] = counter[FN]\n",
    "    matrix.iloc[1, 1] = counter[TN]\n",
    "\n",
    "    accuracy: float = (counter[TP] + counter[TN]) / counter.total()\n",
    "\n",
    "    to_print = [\n",
    "        f\"Iteration {index + 1}\",\n",
    "        f\"Confusion Matrix:\\n\\n{matrix}\",\n",
    "        f\"Accuracy: {accuracy}\"\n",
    "    ]\n",
    "\n",
    "    if epilog is not None:\n",
    "        to_print.append(epilog)\n",
    "\n",
    "    print(\n",
    "        *to_print,\n",
    "        sep = \"\\n\\n\",\n",
    "        end=\"\\n\\n\\n\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bayes(index: int) -> None:\n",
    "    estimator = GaussianNB()\n",
    "    estimate(index, estimator)\n",
    "\n",
    "\n",
    "for i in range(len(cv_data)):\n",
    "    bayes(i)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "K_VALUES = (1, 3, 5)\n",
    "\n",
    "\n",
    "def knn(index: int, k: int) -> None:\n",
    "    estimator = KNeighborsClassifier(n_neighbors=k)\n",
    "    estimate(index, estimator, epilog=f\"k: {k}\")\n",
    "\n",
    "\n",
    "for i in range(len(cv_data)):\n",
    "    for k in K_VALUES:\n",
    "        knn(i, k)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.concat(\n",
    "    (\n",
    "        pd.DataFrame(\n",
    "            scale(data.loc[:, [\"GPA\", \"GMAT\"]]),\n",
    "            columns=[\"GPA\", \"GMAT\"]\n",
    "        ),\n",
    "        data.iloc[:, -1]\n",
    "    ),\n",
    "    axis=1\n",
    ")\n",
    "\n",
    "cv_data: list[dict[str, pd.DataFrame]] = []\n",
    "for itrain, itest in KFold(shuffle=True).split(data):\n",
    "    cv_data.append(\n",
    "        {\n",
    "            \"train_x\": data.loc[itrain, [\"GPA\", \"GMAT\"]],\n",
    "            \"train_cls\": data.loc[itrain, \"De\"],\n",
    "            \"test_x\":data.loc[itest, [\"GPA\", \"GMAT\"]],\n",
    "            \"test_cls\": data.loc[itest, \"De\"]\n",
    "        }\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### bayes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(cv_data)):\n",
    "    bayes(i)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### knn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "K_VALUES = (1, 3, 5)\n",
    "\n",
    "for i in range(len(cv_data)):\n",
    "    for k in K_VALUES:\n",
    "        knn(i, k)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv(\"RidingMowers.csv\")\n",
    "\n",
    "data_own = data[data[\"Ownership\"]==\"Owner\"].iloc[:, :2]\n",
    "data_non = data[data[\"Ownership\"]==\"Nonowner\"].iloc[:, :2]\n",
    "\n",
    "# prior probablity\n",
    "prior_own = data_own.shape[0] / data.shape[0]\n",
    "prior_non = data_non.shape[0] / data.shape[0]\n",
    "\n",
    "# mean\n",
    "mean_own = data_own.mean()\n",
    "mean_non = data_non.mean()\n",
    "\n",
    "# covariance matrix\n",
    "cov_own = data_own.cov()\n",
    "cov_non = data_non.cov()\n",
    "\n",
    "for i in data.index:\n",
    "    x_own = multivariate_normal.pdf(\n",
    "        (data.iloc[i, 0], data.iloc[i, 1]),\n",
    "        mean_own,\n",
    "        np.array(cov_own)\n",
    "    )\n",
    "    x_non = multivariate_normal.pdf(\n",
    "        (data.iloc[i, 0], data.iloc[i, 1]),\n",
    "        mean_non,\n",
    "        np.array(cov_non)\n",
    "    )\n",
    "    post_own = prior_own * x_own\n",
    "    post_non = prior_non * x_non\n",
    "    data.loc[i, \"Prob_Own\"] = post_own / (post_own + post_non)\n",
    "    data.loc[i, \"Prob_Non\"] = post_non / (post_own + post_non)\n",
    "\n",
    "data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### AUC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "CUT_OFF = (0, 0.25, 0.5, 0.75, 1)\n",
    "\n",
    "\n",
    "def foo(cut_off: int) -> tuple[int, int]:\n",
    "    counter = Counter()\n",
    "\n",
    "    for i, j in enumerate(data.loc[:, \"Prob_Own\"]):\n",
    "        if j >= cut_off:\n",
    "            if data.loc[i, \"Ownership\"] == \"Owner\":\n",
    "                counter[TP] += 1\n",
    "            else:\n",
    "                counter[FP] += 1\n",
    "        else:\n",
    "            if data.loc[i, \"Ownership\"] == \"Owner\":\n",
    "                counter[FN] += 1\n",
    "            else:\n",
    "                counter[TN] += 1\n",
    "\n",
    "    sensitivity = counter[TP] / (counter[TP] + counter[FN])\n",
    "    _1_specificity = 1 - (counter[TN] / (counter[TN] + counter[FP]))\n",
    "\n",
    "    return _1_specificity, sensitivity\n",
    "\n",
    "\n",
    "plt.plot(*zip(*[foo(i) for i in CUT_OFF]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "COLUMNS = [\n",
    "    \"CRIM\", \"ZN\", \"INDUS\", \"NOX\", \"RM\",\n",
    "    \"AGE\", \"DIS\", \"TAX\", \"PTRATIO\", \"LSTAT\"\n",
    "]\n",
    "\n",
    "data = pd.read_csv(\"BostonHousing-2.csv\")\n",
    "data[COLUMNS] = (data[COLUMNS] - data[COLUMNS].mean()) / data[COLUMNS].std()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = data.sample(frac=0.6)\n",
    "test_df = data.drop(train_df.index)\n",
    "\n",
    "# isinstance([*train_df.itertuples()][0], tuple)\n",
    "\n",
    "train_x = [i[1:-2] for i in train_df.itertuples()]\n",
    "train_cls = [*train_df[\"MEDV\"]]\n",
    "test_x = [i[1:-2] for i in test_df.itertuples()]\n",
    "test_cls = [*test_df[\"MEDV\"]]\n",
    "\n",
    "for k in range(1, 6):\n",
    "    knn = KNeighborsRegressor(n_neighbors=k)\n",
    "    knn.fit(train_x, train_cls)\n",
    "    predicted_cls = knn.predict(test_x)\n",
    "    RMSE = np.sqrt(metrics.mean_squared_error(test_cls, predicted_cls))\n",
    "    print(f\"RMSE for k = {k} is {RMSE}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Because RMSE for k equal to specific value varies significantly as argument `random_state` in `pandas.DataFrame.sample` method changes, we cannot determine which k value has better performance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new = pd.DataFrame(\n",
    "    [[0.2, 0, 7, 0, 0.538, 6, 62, 4.7, 4, 307, 21, 10]],\n",
    "    columns=[\n",
    "        'CRIM', 'ZN', 'INDUS', 'CHAS', 'NOX', 'RM',\n",
    "        'AGE', 'DIS', 'RAD', 'TAX', 'PTRATIO', 'LSTAT'\n",
    "    ]\n",
    ")\n",
    "new[COLUMNS] = (new[COLUMNS] - data[COLUMNS].mean()) / data[COLUMNS].std()\n",
    "new_x = [i[1:] for i in new.itertuples()]\n",
    "\n",
    "# as we discussed in part a, best k cannot be determined\n",
    "# we hereby assume best k is equal to 3,\n",
    "# which came from my teammate Junxiang Yang\n",
    "knn = KNeighborsRegressor(n_neighbors=3)\n",
    "knn.fit(train_x, train_cls)\n",
    "predicted_cls = knn.predict(new_x)\n",
    "\n",
    "print(f\"MEDV is predicted to be {predicted_cls[0]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predicted_cls = knn.predict(train_x)\n",
    "RMSE = np.sqrt(metrics.mean_squared_error(train_cls, predicted_cls))\n",
    "\n",
    "print(f\"RMSE for training data is {RMSE}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### d\n",
    "\n",
    "- sample size is too small to fit the model.\n",
    "\n",
    "- new data has anomaly value.\n",
    "\n",
    "### e\n",
    "\n",
    "It has a heavy computational burden when comes to a large dataset, because its time complexity is $O(n^2)$.\n",
    "\n",
    "Opreations are as follow:\n",
    "\n",
    "1. compute distantce from data to predict to all data points exist;\n",
    "\n",
    "1. find k combinations with shortest distance;\n",
    "\n",
    "1. find best k value by performance evaluation.\n",
    "\n",
    "## Problem 4\n",
    "\n",
    "### a\n",
    "\n",
    "$$\n",
    "\\begin{align*}\n",
    "estimated\\ profit &= \\$\\ 2500 \\times 1000\\\\\n",
    "&= \\$\\ 2500000\n",
    "\\end{align*}\n",
    "$$\n",
    "\n",
    "### b\n",
    "\n",
    "The first 10%.\n",
    "\n",
    "### c\n",
    "\n",
    "Cut-off value $\\$ 2500$ is equal to the sale effort, which means $1.00$ lift. According to the chart, we should proceed dwon top 500 customers (50%).\n",
    "\n",
    "### d\n",
    "\n",
    "A twp-stage process instead of the later option can be applied to variable new leads and reduce unnecessary commercial cost from people who do not interest in the product, which means greater profit.\n",
    "\n",
    "## Peoblem 5\n",
    "\n",
    "### 1\n",
    "\n",
    "- Training set will be used for training model.\n",
    "\n",
    "- Validation set will be used for obtain the best hyperparameter combination in order to optimize the performance of the model.\n",
    "\n",
    "### 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv(\"BostonHousing-2.csv\")\n",
    "data_x = [i[1:] for i in data[[\"CRIM\", \"CHAS\", \"RM\"]].itertuples()]\n",
    "data_y = [*data[\"MEDV\"]]\n",
    "\n",
    "lr = LinearRegression()\n",
    "lr.fit(data_x, data_y)\n",
    "\n",
    "print(f\"coefficients: {lr.coef_}\\n\\ny-intercept: {lr.intercept_}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predicted = lr.predict([(0.1, 0, 6)])\n",
    "print(f\"MEDV is {predicted[0]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.heatmap(data.corr())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1\n",
    "\n",
    "`INDUS`, `TAX` and `NOX` have strong positive correlations to each other, therefore they are likely to be measuring the same thing.\n",
    "\n",
    "#### 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.corr()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Remove one of `RAD` and `TAX`.\n",
    "\n",
    "#### 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Backward Elimination"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_x = [(*i[1:9], *i[11:13]) for i in data.itertuples()]\n",
    "sm.OLS(data_y, data_x).fit().summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# drop `AGE`\n",
    "data_x = [(*i[:6], *i[7:]) for i in data_x]\n",
    "sm.OLS(data_y, data_x).fit().summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# drop `NOX`\n",
    "data_x = [(*i[:4], *i[5:]) for i in data_x]\n",
    "sm.OLS(data_y, data_x).fit().summary()\n",
    "# all variables below significant value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = LinearRegression()\n",
    "lr.fit(data_x, data_y)\n",
    "\n",
    "data_y = np.array(data_y)\n",
    "predicted = lr.predict(data_x)\n",
    "\n",
    "RMSE_back = np.sqrt(metrics.mean_squared_error(data_y, predicted))\n",
    "MAPE_back = np.mean(np.abs((data_y - predicted) / data_y)) * 100\n",
    "error_back = np.abs(predicted - data_y)\n",
    "Mean_error_back = np.mean(error_back)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Forward Selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bar = lambda: data.itertuples()\n",
    "\n",
    "sm.OLS(\n",
    "    data_y,\n",
    "    [(i.CRIM, ) for i in bar()]\n",
    ").fit().summary()\n",
    "# all p values below 0.05\n",
    "\n",
    "sm.OLS(\n",
    "    data_y,\n",
    "    [(i.CRIM, i.ZN) for i in bar()]\n",
    ").fit().summary()\n",
    "# all p values below 0.05\n",
    "\n",
    "sm.OLS(\n",
    "    data_y,\n",
    "    [(i.CRIM, i.ZN, i.INDUS) for i in bar()]\n",
    ").fit().summary()\n",
    "# all p values below 0.05\n",
    "\n",
    "sm.OLS(\n",
    "    data_y,\n",
    "    [(i.CRIM, i.ZN, i.INDUS, i.CHAS) for i in bar()]\n",
    ").fit().summary()\n",
    "# all p values below 0.05\n",
    "\n",
    "sm.OLS(\n",
    "    data_y,\n",
    "    [(i.CRIM, i.ZN, i.INDUS, i.CHAS, i.NOX) for i in bar()]\n",
    ").fit().summary()\n",
    "# all p values below 0.05\n",
    "\n",
    "sm.OLS(\n",
    "    data_y,\n",
    "    [(i.CRIM, i.ZN, i.INDUS, i.CHAS, i.NOX, i.RM) for i in bar()]\n",
    ").fit().summary()\n",
    "# `ZN` exceeds significant value, drop `RM`\n",
    "\n",
    "sm.OLS(\n",
    "    data_y,\n",
    "    [(i.CRIM, i.ZN, i.INDUS, i.CHAS, i.NOX, i.AGE) for i in bar()]\n",
    ").fit().summary()\n",
    "# `RM` exceeds significant value, drop `AGE`\n",
    "\n",
    "sm.OLS(\n",
    "    data_y,\n",
    "    [(i.CRIM, i.ZN, i.INDUS, i.CHAS, i.NOX, i.DIS) for i in bar()]\n",
    ").fit().summary()\n",
    "# all p values below 0.05\n",
    "\n",
    "sm.OLS(\n",
    "    data_y,\n",
    "    [(i.CRIM, i.ZN, i.INDUS, i.CHAS, i.NOX, i.DIS, i.PTRATIO) for i in bar()]\n",
    ").fit().summary()\n",
    "# `RM` exceeds significant value, drop `PTRATIO`\n",
    "\n",
    "sm.OLS(\n",
    "    data_y,\n",
    "    [(i.CRIM, i.ZN, i.INDUS, i.CHAS, i.NOX, i.DIS, i.LSTAT) for i in bar()]\n",
    ").fit().summary()\n",
    "# all p values below 0.05"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_x = [(i.CRIM, i.ZN, i.INDUS, i.CHAS, i.NOX, i.DIS, i.LSTAT) for i in bar()]\n",
    "\n",
    "lr = LinearRegression()\n",
    "lr.fit(data_x, data_y)\n",
    "\n",
    "predicted = lr.predict(data_x)\n",
    "\n",
    "RMSE_fore = np.sqrt(metrics.mean_squared_error(data_y, predicted))\n",
    "MAPE_fore = np.mean(np.abs(data_y - predicted) / data_y) * 100\n",
    "error_fore = np.abs(predicted - data_y)\n",
    "Mean_error_fore = np.mean(error_fore)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Stepwise Selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# `ZN` is the first variable that exceeds significant value, drop `ZN`\n",
    "\n",
    "# add `AGE`\n",
    "sm.OLS(\n",
    "    data_y,\n",
    "    [(i.CRIM, i.INDUS, i.CHAS, i.NOX, i.RM, i.AGE) for i in bar()]\n",
    ").fit().summary()\n",
    "# `AGE` exceeds, drop\n",
    "\n",
    "# add `DIS`\n",
    "sm.OLS(\n",
    "    data_y,\n",
    "    [(i.CRIM, i.INDUS, i.CHAS, i.NOX, i.RM, i.DIS) for i in bar()]\n",
    ").fit().summary()\n",
    "# all p values below 0.05\n",
    "\n",
    "# add `PTRATIO`\n",
    "sm.OLS(\n",
    "    data_y,\n",
    "    [(i.CRIM, i.INDUS, i.CHAS, i.NOX, i.RM, i.DIS, i.PTRATIO) for i in bar()]\n",
    ").fit().summary()\n",
    "# all p values below 0.05\n",
    "\n",
    "# add `LSTAT`\n",
    "sm.OLS(\n",
    "    data_y,\n",
    "    [(i.CRIM, i.INDUS, i.CHAS, i.NOX, i.RM, i.DIS, i.PTRATIO, i.LSTAT) for i in bar()]\n",
    ").fit().summary()\n",
    "# `NOX` exceeds, drop\n",
    "\n",
    "sm.OLS(\n",
    "    data_y,\n",
    "    [(i.CRIM, i.INDUS, i.CHAS, i.RM, i.DIS, i.PTRATIO, i.LSTAT) for i in bar()]\n",
    ").fit().summary()\n",
    "# all p values below 0.05"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_x = [(i.CRIM, i.INDUS, i.CHAS, i.RM, i.DIS, i.PTRATIO, i.LSTAT) for i in bar()]\n",
    "\n",
    "lr = LinearRegression()\n",
    "lr.fit(data_x, data_y)\n",
    "\n",
    "predicted = lr.predict(data_x)\n",
    "\n",
    "RMSE_step = np.sqrt(metrics.mean_squared_error(data_y, predicted))\n",
    "MAPE_step = np.mean(np.abs((data_y - predicted) / data_y)) * 100\n",
    "error_step = np.abs(predicted - data_y)\n",
    "Mean_error_step = np.mean(error_step)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1, 3)\n",
    "\n",
    "ax[0].hist(error_back)\n",
    "ax[1].hist(error_fore)\n",
    "ax[2].hist(error_step)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(\n",
    "    [\n",
    "        [RMSE_back, MAPE_back, Mean_error_back],\n",
    "        [RMSE_fore, MAPE_fore, Mean_error_fore], \n",
    "        [RMSE_step, MAPE_step, Mean_error_step]\n",
    "    ],\n",
    "    index=[\"Backward\", \"Fordward\", \"Stepwise\"],\n",
    "    columns=[\"RMSE\", \"MAPE\", \"Mean Error\"]\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.3 64-bit (system)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.3"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "89bc4e4d57955752104a90edca27078e178f7cbbec3cd6756359e7c00dfd30f4"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
